\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}2 Implement a Perceptron}{1}{section.1}}
\newlabel{implement-a-perceptron}{{1}{1}{2 Implement a Perceptron}{section.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.0.1}2.1 Weight Update Function}{1}{subsubsection.1.0.1}}
\newlabel{weight-update-function}{{1.0.1}{1}{2.1 Weight Update Function}{subsubsection.1.0.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.0.2}2.2 Permuting Training Samples}{2}{subsubsection.1.0.2}}
\newlabel{permuting-training-samples}{{1.0.2}{2}{2.2 Permuting Training Samples}{subsubsection.1.0.2}{}}
\newlabel{plotting-the-mean-train-and-test-accuracy-of-both-non-permuted-and-permuted-perceptron-classifiers}{{1.0.2}{2}{Plotting the mean train and test accuracy of both non-permuted and permuted perceptron classifiers}{section*.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Plotting the mean train and test accuracy of both non-permuted and permuted perceptron classifiers}{2}{section*.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.0.3}2.3 Averaged Perceptron}{15}{subsubsection.1.0.3}}
\newlabel{averaged-perceptron}{{1.0.3}{15}{2.3 Averaged Perceptron}{subsubsection.1.0.3}{}}
\newlabel{plotting-the-train-and-test-accuracy-of-both-averaged-and-original-perceptron-classifiers}{{1.0.3}{16}{Plotting the train and test accuracy of both averaged and original perceptron classifiers}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Plotting the train and test accuracy of both averaged and original perceptron classifiers}{16}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.0.4}Let's re-work the analysis in equations 4.3 to 4.6 of the texbook to show that the perceptron also performs better on negative examples (y = \IeC {\ensuremath  {-}}1) after a weight update.}{17}{subsubsection.1.0.4}}
\newlabel{lets-re-work-the-analysis-in-equations-4.3-to-4.6-of-the-texbook-to-show-that-the-perceptron-also-performs-better-on-negative-examples-y-1-after-a-weight-update.}{{1.0.4}{17}{Let's re-work the analysis in equations 4.3 to 4.6 of the texbook to show that the perceptron also performs better on negative examples (y = âˆ’1) after a weight update}{subsubsection.1.0.4}{}}
\newlabel{lets-modify-section-4.5s-convergence-proof-in-the-textbook-to-find-the-bound-if-x-has-a-norm-of-at-most-r-instead-of-1.}{{1.0.4}{18}{\texorpdfstring {Let's modify section 4.5's convergence proof in the textbook to find the bound if \(x\) has a norm of at most \(R\) instead of \(1\).}{Let's modify section 4.5's convergence proof in the textbook to find the bound if x has a norm of at most R instead of 1.}}{section*.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Let's modify section 4.5's convergence proof in the textbook to find the bound if \(x\) has a norm of at most \(R\) instead of \(1\).}{18}{section*.3}}
